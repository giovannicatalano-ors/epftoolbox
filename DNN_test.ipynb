{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epftoolbox.models import hyperparameter_optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run C:\\Users\\CTLGNN00C\\Documents\\GitHub\\epftoolbox\\epftoolbox\\data\\_datasets.py\n",
    "%run C:\\Users\\CTLGNN00C\\Documents\\GitHub\\epftoolbox\\epftoolbox\\models\\_dnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/CTLGNN00C/Documents/ENERGY/Progetto MGP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = read_data(path, dataset = \"FR\") # Grouped dataframes (by date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of layers in DNN\n",
    "nlayers = 2\n",
    "\n",
    "# Market under study. If it not one of the standard ones, the file name\n",
    "# has to be provided, where the file has to be a csv file\n",
    "dataset = 'FR'\n",
    "\n",
    "# Number of years (a year is 364 days) in the test dataset.\n",
    "years_test = 2\n",
    "\n",
    "# Optional parameters for selecting the test dataset, if either of them is not provided, \n",
    "# the test dataset is built using the years_test parameter. They should either be one of\n",
    "# the date formats existing in python or a string with the following format\n",
    "# \"%d/%m/%Y %H:%M\"\n",
    "begin_test_date = None\n",
    "end_test_date = None\n",
    "\n",
    "# Boolean that selects whether the validation and training datasets are shuffled\n",
    "shuffle_train = 1\n",
    "\n",
    "# Boolean that selects whether a data augmentation technique for DNNs is used\n",
    "data_augmentation = 0\n",
    "\n",
    "# Boolean that selects whether we start a new hyperparameter optimization or we restart an existing one\n",
    "new_hyperopt = 1\n",
    "\n",
    "# Number of years used in the training dataset for recalibration\n",
    "calibration_window = 4\n",
    "\n",
    "# Unique identifier to read the trials file of hyperparameter optimization\n",
    "experiment_id = 1\n",
    "\n",
    "# Number of iterations for hyperparameter optimization\n",
    "max_evals = 10\n",
    "\n",
    "path_datasets_folder = path\n",
    "path_hyperparameters_folder = path +  \"/experimental_files/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DON'T RUN AGAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check documentation of the hyperparameter_optimizer for each of the function parameters\n",
    "# In this example, we optimize a model for the PJM market.\n",
    "# We consider two directories, one for storing the datasets and the other one for the experimental files.\n",
    "# We start a hyperparameter optimization from scratch. We employ 1500 iterations in hyperopt,\n",
    "# 2 years of test data, a DNN with 2 hidden layers, a calibration window of 4 years,\n",
    "# we avoid data augmentation,  and we provide an experiment_id equal to 1\n",
    "hyperparameter_optimizer(path_datasets_folder=path_datasets_folder, \n",
    "                         path_hyperparameters_folder=path_hyperparameters_folder, \n",
    "                         new_hyperopt=new_hyperopt, max_evals=max_evals, nlayers=nlayers, dataset=dataset, \n",
    "                         years_test=years_test, calibration_window=calibration_window, \n",
    "                         shuffle_train=shuffle_train, data_augmentation=0, experiment_id=experiment_id,\n",
    "                         begin_test_date=begin_test_date, end_test_date=end_test_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pc\n",
    "\n",
    "# Percorso dove è stato salvato il file di ottimizzazione\n",
    "trials_file_path = os.path.join(path_hyperparameters_folder, 'DNN_hyperparameters_nl' + str(nlayers) +\n",
    "                                '_dat' + str(dataset) + '_YT' + str(years_test) + \n",
    "                                '_SF' * shuffle_train + '_DA' * data_augmentation + \n",
    "                                '_CW' + str(calibration_window) + '_' + str(experiment_id))\n",
    "\n",
    "# Caricare il file di trials salvato\n",
    "with open(trials_file_path, \"rb\") as f:\n",
    "    trials = pc.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 2,\n",
       " 'tid': 2,\n",
       " 'spec': None,\n",
       " 'result': {'loss': 5.982590648310619,\n",
       "  'MAE Val': 5.982590648310619,\n",
       "  'MAE Test': 4.683515868772801,\n",
       "  'sMAPE Val': 13.027268609914284,\n",
       "  'sMAPE Test': 13.116365581093339,\n",
       "  'status': 'ok'},\n",
       " 'misc': {'tid': 2,\n",
       "  'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'workdir': None,\n",
       "  'idxs': {'In: Day': [2],\n",
       "   'In: Exog-1 D': [2],\n",
       "   'In: Exog-1 D-1': [2],\n",
       "   'In: Exog-1 D-7': [2],\n",
       "   'In: Exog-2 D': [2],\n",
       "   'In: Exog-2 D-1': [2],\n",
       "   'In: Exog-2 D-7': [2],\n",
       "   'In: Price D-1': [2],\n",
       "   'In: Price D-2': [2],\n",
       "   'In: Price D-3': [2],\n",
       "   'In: Price D-7': [2],\n",
       "   'activation': [2],\n",
       "   'batch_normalization': [2],\n",
       "   'dropout': [2],\n",
       "   'init': [2],\n",
       "   'lambdal1': [2],\n",
       "   'lr': [2],\n",
       "   'neurons1': [2],\n",
       "   'neurons2': [2],\n",
       "   'reg': [2],\n",
       "   'scaleX': [2],\n",
       "   'scaleY': [2],\n",
       "   'seed': [2]},\n",
       "  'vals': {'In: Day': [1],\n",
       "   'In: Exog-1 D': [1],\n",
       "   'In: Exog-1 D-1': [0],\n",
       "   'In: Exog-1 D-7': [0],\n",
       "   'In: Exog-2 D': [0],\n",
       "   'In: Exog-2 D-1': [1],\n",
       "   'In: Exog-2 D-7': [0],\n",
       "   'In: Price D-1': [1],\n",
       "   'In: Price D-2': [1],\n",
       "   'In: Price D-3': [1],\n",
       "   'In: Price D-7': [0],\n",
       "   'activation': [2],\n",
       "   'batch_normalization': [0],\n",
       "   'dropout': [0.24519885788908224],\n",
       "   'init': [3],\n",
       "   'lambdal1': [1.3726708249859099e-05],\n",
       "   'lr': [0.0016857776507671703],\n",
       "   'neurons1': [412.0],\n",
       "   'neurons2': [274.0],\n",
       "   'reg': [1],\n",
       "   'scaleX': [5],\n",
       "   'scaleY': [3],\n",
       "   'seed': [743.0]}},\n",
       " 'exp_key': None,\n",
       " 'owner': None,\n",
       " 'version': 0,\n",
       " 'book_time': datetime.datetime(2024, 9, 19, 7, 11, 57, 719000),\n",
       " 'refresh_time': datetime.datetime(2024, 9, 19, 7, 13, 38, 981000)}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Miglior risultato dato dall'ottimizzatore\n",
    "trials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parametri migliori per la migliore configurazione\n",
    "best_trial = trials.best_trial\n",
    "# best_hyperparams = best_trial['misc']['vals']\n",
    "# best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definizione iperparametri\n",
    "# activation = best_hyperparams['activation'][0]\n",
    "# batch_normalization = best_hyperparams['batch_normalization'][0]\n",
    "# dropout = best_hyperparams['dropout'][0]\n",
    "# learning_rate = best_hyperparams['lr'][0]\n",
    "# neurons1 = int(best_hyperparams['neurons1'][0])  # Converti in int per i neuroni\n",
    "# neurons2 = int(best_hyperparams['neurons2'][0])\n",
    "# scaleX = best_hyperparams['scaleX'][0]\n",
    "# scaleY = best_hyperparams['scaleY'][0]\n",
    "# seed = int(best_hyperparams['seed'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definizione features\n",
    "# features = {}\n",
    "# for feat in list(best_hyperparams.keys())[0:11]:\n",
    "#     features[feat] = best_hyperparams[feat][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_hyperparams = format_best_trial(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'In: Day': 1,\n",
       " 'In: Exog-1 D': 1,\n",
       " 'In: Exog-1 D-1': 0,\n",
       " 'In: Exog-1 D-7': 0,\n",
       " 'In: Exog-2 D': 0,\n",
       " 'In: Exog-2 D-1': 1,\n",
       " 'In: Exog-2 D-7': 0,\n",
       " 'In: Price D-1': 1,\n",
       " 'In: Price D-2': 1,\n",
       " 'In: Price D-3': 1,\n",
       " 'In: Price D-7': 0,\n",
       " 'activation': 'tanh',\n",
       " 'batch_normalization': 0,\n",
       " 'dropout': 0.24519885788908224,\n",
       " 'init': 'glorot_normal',\n",
       " 'lambdal1': 1.3726708249859099e-05,\n",
       " 'lr': 0.0016857776507671703,\n",
       " 'neurons1': 412.0,\n",
       " 'neurons2': 274.0,\n",
       " 'reg': 'l1',\n",
       " 'scaleX': 'Invariant',\n",
       " 'scaleY': 'Std',\n",
       " 'seed': 743.0}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Exogenous 1</th>\n",
       "      <th>Exogenous 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-09 00:00:00</th>\n",
       "      <td>32.542</td>\n",
       "      <td>63065.0</td>\n",
       "      <td>63000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-09 01:00:00</th>\n",
       "      <td>21.549</td>\n",
       "      <td>62715.0</td>\n",
       "      <td>58800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-09 02:00:00</th>\n",
       "      <td>15.711</td>\n",
       "      <td>61952.0</td>\n",
       "      <td>58500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-09 03:00:00</th>\n",
       "      <td>10.583</td>\n",
       "      <td>59262.0</td>\n",
       "      <td>54300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-09 04:00:00</th>\n",
       "      <td>10.324</td>\n",
       "      <td>56883.0</td>\n",
       "      <td>51900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 19:00:00</th>\n",
       "      <td>47.620</td>\n",
       "      <td>77508.0</td>\n",
       "      <td>68739.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 20:00:00</th>\n",
       "      <td>43.790</td>\n",
       "      <td>75506.0</td>\n",
       "      <td>66734.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 21:00:00</th>\n",
       "      <td>42.440</td>\n",
       "      <td>72883.0</td>\n",
       "      <td>64515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 22:00:00</th>\n",
       "      <td>42.030</td>\n",
       "      <td>72926.0</td>\n",
       "      <td>62554.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03 23:00:00</th>\n",
       "      <td>40.910</td>\n",
       "      <td>73070.0</td>\n",
       "      <td>67342.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34944 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Price  Exogenous 1  Exogenous 2\n",
       "Date                                                 \n",
       "2011-01-09 00:00:00  32.542      63065.0      63000.0\n",
       "2011-01-09 01:00:00  21.549      62715.0      58800.0\n",
       "2011-01-09 02:00:00  15.711      61952.0      58500.0\n",
       "2011-01-09 03:00:00  10.583      59262.0      54300.0\n",
       "2011-01-09 04:00:00  10.324      56883.0      51900.0\n",
       "...                     ...          ...          ...\n",
       "2015-01-03 19:00:00  47.620      77508.0      68739.0\n",
       "2015-01-03 20:00:00  43.790      75506.0      66734.0\n",
       "2015-01-03 21:00:00  42.440      72883.0      64515.0\n",
       "2015-01-03 22:00:00  42.030      72926.0      62554.0\n",
       "2015-01-03 23:00:00  40.910      73070.0      67342.0\n",
       "\n",
       "[34944 rows x 3 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From dataframe to dataset (array)\n",
    "* Dataframe: Price, Ex 1 ed Ex 2 aggregate per data/ora\n",
    "* Dataset: Variabili sviluppate ossia ogni riga è un giorno e per ogni riga vengono considerate le variabili ottenute dalla Bayesian Optimization a livello orario (nota che comprende i lag e che i dati iniziano dal lag massimo + 1) \n",
    "\n",
    "\n",
    "train_df.shape[0]/24 - (X_train.shape[0] + X_val.shape[0]) = 7 -> massimo lag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test, indexTest = \\\n",
    "    _build_and_split_XYs(dfTrain=train_df,dfTest=test_df,  features=opt_hyperparams, n_exogenous_inputs=2, shuffle_train=True, percentage_val=0.25,\n",
    "                        date_test=None, hyperoptimization=True, data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2015-01-11', '2015-01-12', '2015-01-13', '2015-01-14',\n",
       "               '2015-01-15', '2015-01-16', '2015-01-17', '2015-01-18',\n",
       "               '2015-01-19', '2015-01-20',\n",
       "               ...\n",
       "               '2016-12-22', '2016-12-23', '2016-12-24', '2016-12-25',\n",
       "               '2016-12-26', '2016-12-27', '2016-12-28', '2016-12-29',\n",
       "               '2016-12-30', '2016-12-31'],\n",
       "              dtype='datetime64[ns]', name='Date', length=721, freq=None)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 2,\n",
       " 'tid': 2,\n",
       " 'spec': None,\n",
       " 'result': {'loss': 5.982590648310619,\n",
       "  'MAE Val': 5.982590648310619,\n",
       "  'MAE Test': 4.683515868772801,\n",
       "  'sMAPE Val': 13.027268609914284,\n",
       "  'sMAPE Test': 13.116365581093339,\n",
       "  'status': 'ok'},\n",
       " 'misc': {'tid': 2,\n",
       "  'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'workdir': None,\n",
       "  'idxs': {'In: Day': [2],\n",
       "   'In: Exog-1 D': [2],\n",
       "   'In: Exog-1 D-1': [2],\n",
       "   'In: Exog-1 D-7': [2],\n",
       "   'In: Exog-2 D': [2],\n",
       "   'In: Exog-2 D-1': [2],\n",
       "   'In: Exog-2 D-7': [2],\n",
       "   'In: Price D-1': [2],\n",
       "   'In: Price D-2': [2],\n",
       "   'In: Price D-3': [2],\n",
       "   'In: Price D-7': [2],\n",
       "   'activation': [2],\n",
       "   'batch_normalization': [2],\n",
       "   'dropout': [2],\n",
       "   'init': [2],\n",
       "   'lambdal1': [2],\n",
       "   'lr': [2],\n",
       "   'neurons1': [2],\n",
       "   'neurons2': [2],\n",
       "   'reg': [2],\n",
       "   'scaleX': [2],\n",
       "   'scaleY': [2],\n",
       "   'seed': [2]},\n",
       "  'vals': {'In: Day': [1],\n",
       "   'In: Exog-1 D': [1],\n",
       "   'In: Exog-1 D-1': [0],\n",
       "   'In: Exog-1 D-7': [0],\n",
       "   'In: Exog-2 D': [0],\n",
       "   'In: Exog-2 D-1': [1],\n",
       "   'In: Exog-2 D-7': [0],\n",
       "   'In: Price D-1': [1],\n",
       "   'In: Price D-2': [1],\n",
       "   'In: Price D-3': [1],\n",
       "   'In: Price D-7': [0],\n",
       "   'activation': [2],\n",
       "   'batch_normalization': [0],\n",
       "   'dropout': [0.24519885788908224],\n",
       "   'init': [3],\n",
       "   'lambdal1': [1.3726708249859099e-05],\n",
       "   'lr': [0.0016857776507671703],\n",
       "   'neurons1': [412.0],\n",
       "   'neurons2': [274.0],\n",
       "   'reg': [1],\n",
       "   'scaleX': [5],\n",
       "   'scaleY': [3],\n",
       "   'seed': [743.0]}},\n",
       " 'exp_key': None,\n",
       " 'owner': None,\n",
       " 'version': 0,\n",
       " 'book_time': datetime.datetime(2024, 9, 19, 7, 11, 57, 719000),\n",
       " 'refresh_time': datetime.datetime(2024, 9, 19, 7, 13, 38, 981000)}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(experiment_id=1, path_hyperparameter_folder=path_hyperparameters_folder, nlayers=2,\n",
    "            dataset='FR', years_test=2, shuffle_train=1, data_augmentation=0, calibration_window=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg, X_val_reg, X_test_reg, Y_train_reg, Y_val_reg = model._regularize_data(X_train, X_val, X_test, Y_train, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model.recalibrate(X_train_reg, Y_train_reg, X_val_reg, Y_val_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721, 24)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,17304) (24,) (1,17304) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_reg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\epftoolbox\\epftoolbox\\models\\_dnn.py:556\u001b[0m, in \u001b[0;36mDNN.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaleY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNorm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNorm1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedian\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvariant\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Yp\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 556\u001b[0m     Yp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mYp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Yp\n",
      "File \u001b[1;32mc:\\Users\\CTLGNN00C\\Documents\\GitHub\\epftoolbox\\epftoolbox\\data\\_wrangling.py:245\u001b[0m, in \u001b[0;36mDataScaler.inverse_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m    230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Method that inverse-scale the data in ``dataset``\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    It must be called after calling the :class:`fit_transform` method for estimating the scaler\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m        Inverse-scaled data\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CTLGNN00C\\AppData\\Local\\anaconda3\\envs\\epftoolbox\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1107\u001b[0m, in \u001b[0;36mStandardScaler.inverse_transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_std:\n\u001b[1;32m-> 1107\u001b[0m         X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n\u001b[0;32m   1109\u001b[0m         X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,17304) (24,) (1,17304) "
     ]
    }
   ],
   "source": [
    "model.predict(X_test_reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epftoolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
